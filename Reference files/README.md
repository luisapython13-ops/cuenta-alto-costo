# ğŸš¢ Proyecto Titanic: PredicciÃ³n de Supervivencia

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3+-orange.svg)](https://scikit-learn.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.50+-red.svg)](https://streamlit.io/)

Proyecto completo de Machine Learning end-to-end para predecir la supervivencia de pasajeros del Titanic. Incluye exploraciÃ³n de datos, entrenamiento con AutoML, interpretabilidad de modelos y deployment con Streamlit.

---

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n](#-descripciÃ³n)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Pipeline del Proyecto](#-pipeline-del-proyecto)
- [InstalaciÃ³n y Uso](#-instalaciÃ³n-y-uso)
- [Resultados y MÃ©tricas](#-resultados-y-mÃ©tricas)
- [Interpretabilidad](#-interpretabilidad)
- [Deployment](#-deployment)
- [TecnologÃ­as](#-tecnologÃ­as)

---

## ğŸ“– DescripciÃ³n

Este proyecto demuestra un **flujo completo de Machine Learning** utilizando el famoso dataset del Titanic. El objetivo es predecir si un pasajero sobreviviÃ³ al desastre basÃ¡ndose en caracterÃ­sticas como edad, gÃ©nero, clase, tarifa, y nÃºmero de familiares a bordo.

### Â¿QuÃ© AprenderÃ¡s?

- âœ… Descarga y limpieza de datos desde OpenML
- âœ… AnÃ¡lisis exploratorio de datos (EDA)
- âœ… Preprocesamiento con pipelines de scikit-learn
- âœ… ComparaciÃ³n de modelos con AutoML (PyCaret y FLAML)
- âœ… OptimizaciÃ³n de hiperparÃ¡metros con GridSearchCV
- âœ… InterpretaciÃ³n de modelos con Feature Importance
- âœ… Deployment de modelos con Streamlit

### Dataset

- **Fuente:** OpenML ([Dataset 1309](https://www.openml.org/d/1309))
- **TamaÃ±o:** 1,309 pasajeros
- **CaracterÃ­sticas:** 9 variables (despuÃ©s de limpieza)
- **Target:** `survived` (0 = No sobreviviÃ³, 1 = SobreviviÃ³)

---

## ğŸ“ Estructura del Proyecto

```
projects/titanic/
â”œâ”€â”€ 01_descarga_datos.ipynb                    # Descarga y preparaciÃ³n inicial
â”œâ”€â”€ 02_ajuste_datos.ipynb                      # Limpieza y transformaciÃ³n
â”œâ”€â”€ 04_AutoML_PyCaret.ipynb                    # ExploraciÃ³n con PyCaret
â”œâ”€â”€ 05_AutoML_FLAML.ipynb                      # ExploraciÃ³n con FLAML
â”œâ”€â”€ 06_entrenamiento_modelo.ipynb              # Entrenamiento y optimizaciÃ³n
â”œâ”€â”€ 07_interpretacion_modelos.ipynb            # Interpretabilidad
â”œâ”€â”€ 08_titanic_streamlit.py                    # App de deployment
â”œâ”€â”€ modelos/
â”‚   â”œâ”€â”€ gbc_tuned_model.pkl                    # Modelo GBC (PyCaret)
â”‚   â”œâ”€â”€ gbc_tuned_model.joblib                 # Modelo GBC (joblib)
â”‚   â””â”€â”€ titanic_classification-random_forest-v1.joblib  # Modelo final
â””â”€â”€ imagenes/
    â””â”€â”€ titanic.jpg                            # Imagen para Streamlit
```

---

## ğŸ”„ Pipeline del Proyecto

### 1ï¸âƒ£ Descarga de Datos (`01_descarga_datos.ipynb`)

**Objetivo:** Obtener el dataset inicial desde OpenML

- Descarga 1,309 registros con 14 columnas
- Identifica y elimina columnas con **data leakage** (`boat`, `body`)
- Selecciona 12 caracterÃ­sticas relevantes
- Exporta en formato **Parquet** para eficiencia

**CaracterÃ­sticas Iniciales:**
- `pclass`, `survived`, `name`, `sex`, `age`, `sibsp`, `parch`, `ticket`, `fare`, `cabin`, `embarked`, `home.dest`

---

### 2ï¸âƒ£ Ajuste de Datos (`02_ajuste_datos.ipynb`)

**Objetivo:** Limpieza y preparaciÃ³n para modelado

**Transformaciones:**
- Reemplaza placeholders "?" por `NaN`
- Elimina columnas con alta proporciÃ³n de nulos: `cabin`, `ticket`, `home.dest`
- Convierte tipos de datos:
  - **CategÃ³ricas ordinales:** `pclass` (1, 2, 3)
  - **CategÃ³ricas nominales:** `sex`, `embarked`
  - **NumÃ©ricas continuas:** `age`, `fare`
  - **NumÃ©ricas discretas:** `sibsp`, `parch`
  - **Booleana:** `survived`

**Dataset Final:** 1,309 filas Ã— 9 columnas

---

### 3ï¸âƒ£ AutoML con PyCaret (`04_AutoML_PyCaret.ipynb`)

**Objetivo:** ExploraciÃ³n rÃ¡pida de mÃºltiples modelos

**Proceso:**
1. Setup automÃ¡tico con preprocesamiento integrado
2. ComparaciÃ³n de 4 algoritmos: LogisticRegression, RandomForest, GBC, LightGBM
3. Fine-tuning del mejor modelo (Gradient Boosting Classifier)

**Resultados:**
- Mejor modelo: **Gradient Boosting Classifier**
- Accuracy: ~82% en validaciÃ³n interna
- Modelo guardado: `gbc_tuned_model.pkl`

---

### 4ï¸âƒ£ AutoML con FLAML (`05_AutoML_FLAML.ipynb`)

**Objetivo:** OptimizaciÃ³n eficiente con presupuesto de tiempo

**Diferencias vs PyCaret:**
- Preprocesamiento manual con `ColumnTransformer`
- BÃºsqueda econÃ³mica de hiperparÃ¡metros (CFO algorithm)
- Mayor control y transparencia

**Proceso:**
1. Split 80/20 (1,047 entrenamiento / 262 prueba)
2. Pipeline de preprocesamiento personalizado
3. BÃºsqueda automÃ¡tica en 60 segundos
4. Modelos evaluados: LGBM, RF, XGBoost, ExtraTrees, LogisticRegression

**Resultados:**
- Mejor modelo: **LightGBM**
- Accuracy test: **84.35%**
- Tiempo de bÃºsqueda: ~21 segundos

---

### 5ï¸âƒ£ Entrenamiento Final (`06_entrenamiento_modelo.ipynb`)

**Objetivo:** Seleccionar, optimizar y guardar el modelo de producciÃ³n

**Modelos Evaluados:**

| Pipeline | Accuracy | F1 | ROC-AUC |
|----------|----------|-----|---------|
| **RandomForest_median** | **0.7489** | **0.667** | **0.728** |
| LogisticRegression_mean_scale | 0.7444 | 0.689 | 0.735 |
| LogisticRegression_median | 0.7354 | 0.681 | 0.727 |
| RandomForest_mean_scale | 0.7265 | 0.643 | 0.707 |

**Modelo Seleccionado:** RandomForest con imputaciÃ³n por mediana

**Razones:**
- âœ… Robustez contra outliers
- âœ… Consistencia en validaciÃ³n cruzada (5-folds)
- âœ… Simplicidad e interpretabilidad
- âœ… Mejor generalizaciÃ³n

**HiperparÃ¡metros Optimizados (GridSearchCV):**
```python
{
    'n_estimators': 50,
    'max_depth': None,
    'min_samples_split': 10
}
```

**Modelo Guardado:** `titanic_classification-random_forest-v1.joblib` (754 KB)

---

### 6ï¸âƒ£ InterpretaciÃ³n (`07_interpretacion_modelos.ipynb`)

**Objetivo:** Entender quÃ© aprende el modelo

**Feature Importance (Top 7):**

| Variable | Importancia | DescripciÃ³n |
|----------|-------------|-------------|
| **fare** | 21.3% | Tarifa del boleto (proxy de clase socioeconÃ³mica) |
| **age** | 19.2% | Edad del pasajero |
| **sex_female** | 19.1% | GÃ©nero femenino |
| **sex_male** | 17.1% | GÃ©nero masculino |
| **pclass** | 12.5% | Clase del boleto (1Âª, 2Âª, 3Âª) |
| **parch** | 4.3% | NÃºmero de padres/hijos a bordo |
| **sibsp** | 3.5% | NÃºmero de hermanos/cÃ³nyuge a bordo |

**Insights Clave:**
- ğŸ’° **Tarifa y clase** son los factores mÃ¡s influyentes (refleja desigualdad socioeconÃ³mica)
- ğŸ‘¥ **GÃ©nero** es crÃ­tico (polÃ­tica "mujeres y niÃ±os primero")
- ğŸ“Š **Edad** tambiÃ©n importante (niÃ±os fueron priorizados)
- ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ **Familiares** tienen menor impacto

---

### 7ï¸âƒ£ Deployment (`08_titanic_streamlit.py`)

**Objetivo:** App web interactiva para predicciones en tiempo real

**CaracterÃ­sticas:**
- Interfaz intuitiva con Streamlit
- Entrada de 7 caracterÃ­sticas del pasajero
- PredicciÃ³n instantÃ¡nea con modelo cargado
- VisualizaciÃ³n con imagen histÃ³rica del Titanic

**Inputs del Usuario:**
```
NumÃ©ricos:
  - Age: 0-100 aÃ±os
  - Fare: 0-300 (costo del boleto)
  - SibSp: 0-15 (hermanos/cÃ³nyuge)
  - Parch: 0-15 (padres/hijos)

CategÃ³ricos:
  - Pclass: 1st / 2nd / 3rd
  - Sex: Woman / Man
  - Embarked: Cherbourg / Queenstown / Southampton
```

---

## ğŸš€ InstalaciÃ³n y Uso

### Prerrequisitos

- Python 3.11
- [uv](https://github.com/astral-sh/uv) (recomendado)
- Jupyter Notebook/Lab

### InstalaciÃ³n

```bash
# Desde la raÃ­z del repositorio
cd data-projects-lab

# Instalar dependencias base
uv sync

# Instalar dependencias opcionales (AutoML, interpretabilidad)
uv pip install -e ".[all]"
```

### Ejecutar Notebooks

```bash
# Iniciar Jupyter Lab
uv run jupyter lab

# Navegar a projects/titanic/
# Ejecutar en orden: 01 â†’ 02 â†’ 04/05 â†’ 06 â†’ 07
```

### Ejecutar Streamlit App

```bash
# OpciÃ³n 1: Con uv (recomendado)
uv run streamlit run projects/titanic/08_titanic_streamlit.py

# OpciÃ³n 2: Con entorno virtual activado
source .venv/bin/activate
streamlit run projects/titanic/08_titanic_streamlit.py

# La app se abrirÃ¡ en http://localhost:8501
```

### Usar el Modelo en Python

```python
import joblib
import pandas as pd

# Cargar modelo
model = joblib.load("projects/titanic/modelos/titanic_classification-random_forest-v1.joblib")

# Crear datos de entrada
data = pd.DataFrame({
    'age': [25],
    'fare': [50.0],
    'sibsp': [1],
    'parch': [0],
    'pclass': [2],
    'sex': ['female'],
    'embarked': ['C']
})

# Predecir
prediction = model.predict(data)
print(f"Sobrevive: {'SÃ­' if prediction[0] == 1 else 'No'}")
```

---

## ğŸ“Š Resultados y MÃ©tricas

### Modelo Final: RandomForest_median

**ValidaciÃ³n Cruzada (5-folds):**
```
Media Accuracy: 80.36%
  - Fold 1: 70.95%
  - Fold 2: 75.84%
  - Fold 3: 79.78%
  - Fold 4: 77.53%
  - Fold 5: 78.09%
```

**Test Set:**
```
Accuracy:  74.89%
Precision: 74.67%
Recall:    60.22%
F1-Score:  66.67%
ROC-AUC:   0.728
```

**InterpretaciÃ³n:**
- El modelo tiene **buena precisiÃ³n** (evita falsos positivos)
- **Recall moderado** (mÃ¡s conservador en predicciones)
- **Generaliza bien** con varianza baja entre folds

---

## ğŸ” Interpretabilidad

### Factores Clave de Supervivencia

1. **Tarifa del Boleto (21.3%):** Proxy de riqueza y acceso a mejores ubicaciones
2. **Edad (19.2%):** NiÃ±os fueron priorizados en evacuaciÃ³n
3. **GÃ©nero (36.2% combinado):** PolÃ­tica "mujeres y niÃ±os primero"
4. **Clase (12.5%):** Primera clase tuvo mejor acceso a botes salvavidas

### Patrones Aprendidos

El modelo refleja la **desigualdad socioeconÃ³mica** y las **polÃ­ticas de evacuaciÃ³n** del Titanic:
- Pasajeros de primera clase (tarifa alta) tuvieron mejor supervivencia
- Mujeres y niÃ±os fueron priorizados
- Familias con hijos (parch > 0) tuvieron ligera ventaja

---

## ğŸš€ Deployment

### Arquitectura Streamlit

```
Usuario â†’ Streamlit UI â†’ get_user_data() â†’ DataFrame
                             â†“
                   load_model() (cached)
                             â†“
              Pipeline.predict(user_data)
                             â†“
         Preprocesamiento + RandomForest
                             â†“
                PredicciÃ³n: 0 o 1
                             â†“
              Muestra resultado con emoji
```

### TecnologÃ­as de Deployment

- **Streamlit:** Framework web interactivo
- **Joblib:** SerializaciÃ³n eficiente de modelos
- **scikit-learn Pipeline:** Preprocesamiento automÃ¡tico

---

## ğŸ› ï¸ TecnologÃ­as

### LibrerÃ­as Principales

| CategorÃ­a | Herramientas |
|-----------|-------------|
| **Data Science** | pandas, numpy, scikit-learn |
| **VisualizaciÃ³n** | matplotlib, seaborn, plotly |
| **AutoML** | PyCaret (>=3.3.0), FLAML (>=2.3.6) |
| **Deployment** | Streamlit (>=1.50.0) |
| **SerializaciÃ³n** | joblib, pickle |
| **Formato de Datos** | PyArrow (Parquet) |

### Python Version

- **Python:** 3.11

---

## ğŸ“ Valor Educativo

Este proyecto es ideal para aprender:

1. **Flujo Completo de ML:** Desde datos crudos hasta modelo en producciÃ³n
2. **AutoML vs Manual:** ComparaciÃ³n entre PyCaret, FLAML y scikit-learn
3. **Mejores PrÃ¡cticas:** Pipelines, validaciÃ³n cruzada, test/train split
4. **Interpretabilidad:** Entender quÃ© aprende el modelo
5. **Deployment:** Poner modelos en producciÃ³n con Streamlit

---

## ğŸ“ PrÃ³ximos Pasos

Posibles extensiones del proyecto:

- [ ] Agregar tÃ©cnicas de interpretabilidad avanzadas (SHAP, LIME)
- [ ] Implementar Feature Engineering adicional
- [ ] Probar modelos de ensemble (stacking, blending)
- [ ] Agregar anÃ¡lisis de error detallado
- [ ] Deploy en Streamlit Cloud o Hugging Face Spaces
- [ ] Crear API REST con FastAPI

---

## ğŸ‘¤ Autor

**David Palacio JimÃ©nez**

- ğŸ“§ Email: davidpalacioj@gmail.com
- ğŸ™ GitHub: [dpalacioj](https://github.com/dpalacioj)

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver [LICENSE](../../LICENSE) para mÃ¡s detalles.

**Copyright (c) 2025 David Palacio JimÃ©nez**

---

â­ï¸ **Si este proyecto te resulta Ãºtil, considera darle una estrella al repositorio!**

ğŸš€ **Â¡Feliz aprendizaje!**
